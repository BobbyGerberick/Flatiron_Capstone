{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voter Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "As my capstone project for Flatiron School’s Data Science program I built a model to predict how individuals would vote in a presidential election based on data from the 2012, 2016 and 2020 elections. I then used that model to analyze how broad categories of political issues and individual issues themselves influence an individual’s vote. I also examined the accuracy of predictions based on basic demographic information like income, race, education etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "This type of modeling could be useful in a number of contexts. Most obviously for a campaign interested in focusing their efforts on individuals most likely to vote for them but it could also be useful for political parties and special interest groups who want to better understand their constituents and the public as a whole. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "My data comes from the American National Election Studies for the years 2012, 2016 and 2020. The ANES is a national survey of voters in the United States, conducted before and after every presidential election. I used a subset of that data curated by the Inter-university Consortium for Political and Social Research. \n",
    "The full ANES survey data is publicly available for download from here: https://electionstudies.org/. You do have to make an account to access the data which you can do by clicking the login button in the top right corner of the home page. Once you have completed that process click on the Data Center tab at the top of the home page, then select the data set you would like (For example: 2020 Time Series Study) and then under the download data heading on the next page select the type of file you would like. \n",
    "The Inter-university Consortium for Political and Social Research’s data is available here: https://www.icpsr.umich.edu/web/pages/instructors/setups2020/ to individuals with an email address from with one of their member institutions. Once you have made an account with that email address, click on the “Find Data” tab at the top of the page and search for the data set i.e (Voting Behavior: The 2020 Election, Voting Behavior: The 2016 Election, or Voting Behavior: The 2012 Election). The first result will take you to a page where you can download the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "To prepare my data for modeling I first dropped all rows where individuals did not vote or voted for a third party candidate. This left me with 6075 rows to work with. The columns are broken into 16 categories denoted by a letter in front of the question number. For example A01 and R15. Questions in categories A, D and E relate to past political behavior and opinions of current and former politicians. These are obviously strongly correlated with vote preference and are uninteresting in terms of analysis so were dropped. The data is categorical and so needed to be encoded. I used One Hot Encoding to avoid imposing a hierarchy where none should exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, RandomizedSearchCV, train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from kmodes.kmodes import KModes\n",
    "\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',504)\n",
    "pd.set_option('display.width',1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data and displaying first 5 rows\n",
    "df2020 = pd.read_stata('data/SETUPS2020/SETUPS2020.dta')\n",
    "# df2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.611133\n",
       "1       1.209783\n",
       "2       0.823936\n",
       "3       0.512837\n",
       "4       0.856575\n",
       "          ...   \n",
       "7448    1.480103\n",
       "7449    1.503653\n",
       "7450    1.150732\n",
       "7451    0.281583\n",
       "7452    0.432413\n",
       "Name: WEIGHT, Length: 7453, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2020['WEIGHT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2016 = pd.read_stata('data/SETUPS2016/SETUPS2016.dta')\n",
    "# df2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2012 = pd.read_stata('data/SETUPS2012/SETUPS2012.dta')\n",
    "# df2012.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2020.drop(['CASEID','WEIGHT'], axis=1, inplace=True)\n",
    "df2016.drop(['CASEID','WEIGHT'], axis=1, inplace=True)\n",
    "df2012.drop(['CASEID','WEIGHT_FULL'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Taking a look at the first column which askes if the respondent voted\n",
    "# df2020['A01'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # The second question asks who the respondent voted for\n",
    "# df2020['A02'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Subsetting data to keep only rows where the respondent voted for Donald Trump or Joe Biden\n",
    "# df2020 = df2020.loc[(df2020['A01'] == '1. Voted') & ((df2020['A02'] == '1. Joe Biden') | (df2020['A02'] == '2. Donald Trump'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df2016['A02'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Subsetting data to keep only rows where the respondent voted for Donald Trump or Joe Biden\n",
    "# df2016subset = df2016.loc[(df2016['A02'] == 'Clinton') | (df2016['A02'] == 'Trump')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df2016subset['A02'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df2012['A02'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Subsetting data to keep only rows where the respondent voted for Donald Trump or Joe Biden\n",
    "# df2012subset = df2012.loc[(df2012['A02'] == 'Obama') | (df2012['A02'] == 'Romney')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df2012subset['A02'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting target\n",
    "# y = df2020['A02']\n",
    "# X = df2020.drop(['A02'], axis=1, errors = \"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote a function to get the question categories for my dataset. This will help with subsetting the data later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a dictionary where the key is the question category and the associated value is a list of \n",
    "# Columns in that category\n",
    "def get_columns(df):\n",
    "    # Creating empyt  dictionary\n",
    "    dictionary = {}\n",
    "    # Looping through potential categories \n",
    "    alphabet = list(string.ascii_uppercase[0:26])\n",
    "    for char in alphabet:\n",
    "        # Creating dictionary entry\n",
    "        dictionary[char] = []\n",
    "        for num in list(range(df.shape[1])):\n",
    "            if df.columns[num].startswith(char):\n",
    "                # Populating dictionary entry\n",
    "                dictionary[char].append(df.columns[num])            \n",
    "        temp = dictionary.pop(char)\n",
    "        # Removing keys where the value is empty\n",
    "        if temp != []:\n",
    "            dictionary[char] = temp\n",
    "    # Returning dictionary\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting question categories for the 2020 dataset\n",
    "_2020_dictionary = get_columns(df2020)\n",
    "_2016_dictionary = get_columns(df2016)\n",
    "_2012_dictionary = get_columns(df2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping question categories\n",
    "df2020.drop(_2020_dictionary['A'], axis=1, inplace=True, errors = \"ignore\")\n",
    "df2020.drop(_2020_dictionary['D'], axis=1, inplace=True)\n",
    "df2020.drop(_2020_dictionary['E'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2020_dictionary.pop('A')\n",
    "_2020_dictionary.pop('D')\n",
    "_2020_dictionary.pop('E');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping question categories\n",
    "df2016subset = df2016\n",
    "df2016subset.drop(_2016_dictionary['A'], axis=1, inplace=True, errors = \"ignore\")\n",
    "df2016subset.drop(_2016_dictionary['D'], axis=1, inplace=True)\n",
    "df2016subset.drop(_2016_dictionary['E'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2016_dictionary.pop('A')\n",
    "_2016_dictionary.pop('D')\n",
    "_2016_dictionary.pop('E');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2012subset = df2012\n",
    "df2012subset.drop(_2012_dictionary['A'], axis=1, inplace=True, errors = \"ignore\")\n",
    "df2012subset.drop(_2012_dictionary['D'], axis=1, inplace=True)\n",
    "df2012subset.drop(_2012_dictionary['E'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2012_dictionary.pop('A')\n",
    "_2012_dictionary.pop('D')\n",
    "_2012_dictionary.pop('E');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_columns = list(df2020.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preforming train/test split\n",
    "# X = X[categorical_columns]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2020 data requires less preprocessing because of the way its formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension data: 7453 rows and 204 columns\n"
     ]
    }
   ],
   "source": [
    "# The dimension of data\n",
    "print('Dimension data: {} rows and {} columns'.format(len(df2020), len(df2020.columns)))\n",
    "# Print the first 5 rows\n",
    "df2020.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 1, iteration: 1/100, moves: 3056, cost: 698635.0\n",
      "Run 1, iteration: 2/100, moves: 1082, cost: 696973.0\n",
      "Run 1, iteration: 3/100, moves: 527, cost: 696441.0\n",
      "Run 1, iteration: 4/100, moves: 248, cost: 696317.0\n",
      "Run 1, iteration: 5/100, moves: 171, cost: 696257.0\n",
      "Run 1, iteration: 6/100, moves: 19, cost: 696257.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 2, iteration: 1/100, moves: 2648, cost: 709211.0\n",
      "Run 2, iteration: 2/100, moves: 1223, cost: 704728.0\n",
      "Run 2, iteration: 3/100, moves: 895, cost: 702214.0\n",
      "Run 2, iteration: 4/100, moves: 717, cost: 700972.0\n",
      "Run 2, iteration: 5/100, moves: 388, cost: 700745.0\n",
      "Run 2, iteration: 6/100, moves: 160, cost: 700720.0\n",
      "Run 2, iteration: 7/100, moves: 25, cost: 700720.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 3, iteration: 1/100, moves: 2780, cost: 698968.0\n",
      "Run 3, iteration: 2/100, moves: 721, cost: 698219.0\n",
      "Run 3, iteration: 3/100, moves: 275, cost: 698042.0\n",
      "Run 3, iteration: 4/100, moves: 131, cost: 697992.0\n",
      "Run 3, iteration: 5/100, moves: 27, cost: 697992.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 4, iteration: 1/100, moves: 3007, cost: 699957.0\n",
      "Run 4, iteration: 2/100, moves: 932, cost: 699198.0\n",
      "Run 4, iteration: 3/100, moves: 276, cost: 699154.0\n",
      "Run 4, iteration: 4/100, moves: 149, cost: 698955.0\n",
      "Run 4, iteration: 5/100, moves: 248, cost: 698376.0\n",
      "Run 4, iteration: 6/100, moves: 264, cost: 698192.0\n",
      "Run 4, iteration: 7/100, moves: 128, cost: 698130.0\n",
      "Run 4, iteration: 8/100, moves: 100, cost: 697990.0\n",
      "Run 4, iteration: 9/100, moves: 97, cost: 697963.0\n",
      "Run 4, iteration: 10/100, moves: 22, cost: 697963.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 5, iteration: 1/100, moves: 2612, cost: 697935.0\n",
      "Run 5, iteration: 2/100, moves: 735, cost: 697069.0\n",
      "Run 5, iteration: 3/100, moves: 232, cost: 696946.0\n",
      "Run 5, iteration: 4/100, moves: 129, cost: 696877.0\n",
      "Run 5, iteration: 5/100, moves: 41, cost: 696877.0\n",
      "Best run was number 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 4, 5, ..., 0, 1, 5], dtype=uint16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmode2020 = KModes(n_clusters=6, init = \"random\", n_init = 5, verbose=1) \n",
    "kmode2020.fit_predict(df2020) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2020['Cluster'] = kmode2020.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2016 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the categories in 2016 and 2012 mix numeric and string data so we will need to deal with that before doing the cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2016_dictionary;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J, K, L, M, N, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2016_test_subset = df2016subset[df2016subset.columns[pd.Series(df2016subset.columns).str.startswith('R')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Low commitment\n",
       "1                     2\n",
       "2                     3\n",
       "3                     4\n",
       "4                     3\n",
       "             ...       \n",
       "3644                  3\n",
       "3645                  3\n",
       "3646                  2\n",
       "3647                  3\n",
       "3648    High commitment\n",
       "Name: K08, Length: 3649, dtype: category\n",
       "Categories (6, object): ['High commitment' < 2 < 3 < 4 < 'Low commitment' < 'NA']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2016subset['K08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2016subset['H01'].cat.rename_categories(['Government insurance plan','2','3','4','5','6','Private insurance plan','NA'], inplace = True)\n",
    "df2016subset['H02'].cat.rename_categories(['Government insurance plan','2','3','4','5','6','Private insurance plan','NA'], inplace = True)\n",
    "df2016subset['H03'].cat.rename_categories(['Government insurance plan','2','3','4','5','6','Private insurance plan','NA'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2016subset['J01'].cat.rename_categories(['Provide many fewer services','2','3','4','5','6','Provide many more services','NA'], inplace = True)\n",
    "df2016subset['J02'].cat.rename_categories(['Provide many fewer services','2','3','4','5','6','Provide many more services','NA'], inplace = True)\n",
    "df2016subset['J03'].cat.rename_categories(['Provide many fewer services','2','3','4','5','6','Provide many more services','NA'], inplace = True)\n",
    "df2016subset['J04'].cat.rename_categories(['Government should see to it','2','3','4','5','6','Individuals on own','NA'], inplace = True)\n",
    "df2016subset['J05'].cat.rename_categories(['Government should see to it','2','3','4','5','6','Individuals on own','NA'], inplace = True)\n",
    "df2016subset['J06'].cat.rename_categories(['Government should see to it','2','3','4','5','6','Individuals on own','NA'], inplace = True)\n",
    "df2016subset['J14'].cat.rename_categories(['Regulate business','2','3','4','5','6','Do not regulate', 'NA'], inplace = True)\n",
    "df2016subset['J15'].cat.rename_categories(['Regulate business','2','3','4','5','6','Do not regulate', 'NA'], inplace = True)\n",
    "df2016subset['J16'].cat.rename_categories(['Regulate business','2','3','4','5','6','Do not regulate', 'NA'], inplace = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2016subset['K07'].cat.rename_categories(['High traditionalism','2','3','4','Low traditionalism','NA'], inplace = True)\n",
    "df2016subset['K08'].cat.rename_categories(['High commitment','2','3','4','Low commitment','NA'], inplace = True);\n",
    "df2016subset['K09'].cat.rename_categories(['Low tolerance','2','3','4','High tolerance','NA'], inplace = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2016subset['L06'].cat.rename_categories(['High','2','3','4','Low','NA'], inplace = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2016subset['M01'].cat.rename_categories(['Govt should help blacks','2','3','4','5','6','Blacks should help themselves', 'NA'], inplace = True);\n",
    "df2016subset['M02'].cat.rename_categories(['Govt should help blacks','2','3','4','5','6','Blacks should help themselves', 'NA'], inplace = True);\n",
    "df2016subset['M03'].cat.rename_categories(['Govt should help blacks','2','3','4','5','6','Blacks should help themselves', 'NA'], inplace = True);\n",
    "df2016subset['M06'].cat.rename_categories(['Low support','2','3','4','High support','NA'], inplace = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2016subset['N05'].cat.rename_categories(['Greatly decrease','2','3','4','5','6','Greatly increase','NA'], inplace = True);\n",
    "df2016subset['N06'].cat.rename_categories(['Greatly decrease','2','3','4','5','6','Greatly increase','NA'], inplace = True);\n",
    "df2016subset['N07'].cat.rename_categories(['Greatly decrease','2','3','4','5','6','Greatly increase','NA'], inplace = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2016subset['P01'].cat.rename_categories(['High Trust','2','3','Low Trust','NA'], inplace = True);\n",
    "df2016subset['P02'].cat.rename_categories(['High efficacy','2','3','Low efficacy','NA'], inplace = True);\n",
    "df2016subset['P03'].cat.rename_categories(['High support','2','3','Low support','NA'], inplace = True);\n",
    "df2016subset['P06'].cat.rename_categories(['High Trust','2','3','Low Trust','NA'], inplace = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost = [] \n",
    "# K = range(6,7) \n",
    "# for k in list(K): \n",
    "#     kmode = KModes(n_clusters=k, init = \"random\", n_init = 5, verbose=1) \n",
    "#     kmode.fit_predict(df2016subset) \n",
    "#     cost.append(kmode.cost_) \n",
    "\n",
    "# plt.plot(K, cost, 'x-') \n",
    "# plt.xlabel('No. of clusters') \n",
    "# plt.ylabel('Cost') \n",
    "# plt.title('Elbow Curve') \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 1, iteration: 1/100, moves: 1707, cost: 272664.0\n",
      "Run 1, iteration: 2/100, moves: 549, cost: 271626.0\n",
      "Run 1, iteration: 3/100, moves: 344, cost: 271331.0\n",
      "Run 1, iteration: 4/100, moves: 167, cost: 271268.0\n",
      "Run 1, iteration: 5/100, moves: 104, cost: 271228.0\n",
      "Run 1, iteration: 6/100, moves: 15, cost: 271228.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 2, iteration: 1/100, moves: 1504, cost: 273229.0\n",
      "Run 2, iteration: 2/100, moves: 574, cost: 272177.0\n",
      "Run 2, iteration: 3/100, moves: 336, cost: 271884.0\n",
      "Run 2, iteration: 4/100, moves: 100, cost: 271870.0\n",
      "Run 2, iteration: 5/100, moves: 20, cost: 271870.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 3, iteration: 1/100, moves: 1645, cost: 273475.0\n",
      "Run 3, iteration: 2/100, moves: 701, cost: 272376.0\n",
      "Run 3, iteration: 3/100, moves: 298, cost: 272065.0\n",
      "Run 3, iteration: 4/100, moves: 138, cost: 272040.0\n",
      "Run 3, iteration: 5/100, moves: 47, cost: 272003.0\n",
      "Run 3, iteration: 6/100, moves: 72, cost: 271967.0\n",
      "Run 3, iteration: 7/100, moves: 4, cost: 271967.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 4, iteration: 1/100, moves: 1671, cost: 273502.0\n",
      "Run 4, iteration: 2/100, moves: 449, cost: 273120.0\n",
      "Run 4, iteration: 3/100, moves: 215, cost: 273021.0\n",
      "Run 4, iteration: 4/100, moves: 187, cost: 272714.0\n",
      "Run 4, iteration: 5/100, moves: 251, cost: 271992.0\n",
      "Run 4, iteration: 6/100, moves: 236, cost: 271871.0\n",
      "Run 4, iteration: 7/100, moves: 137, cost: 271664.0\n",
      "Run 4, iteration: 8/100, moves: 147, cost: 271494.0\n",
      "Run 4, iteration: 9/100, moves: 131, cost: 271389.0\n",
      "Run 4, iteration: 10/100, moves: 185, cost: 271178.0\n",
      "Run 4, iteration: 11/100, moves: 142, cost: 271074.0\n",
      "Run 4, iteration: 12/100, moves: 44, cost: 271074.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 5, iteration: 1/100, moves: 1512, cost: 270722.0\n",
      "Run 5, iteration: 2/100, moves: 510, cost: 269991.0\n",
      "Run 5, iteration: 3/100, moves: 204, cost: 269847.0\n",
      "Run 5, iteration: 4/100, moves: 71, cost: 269847.0\n",
      "Best run was number 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 2, ..., 2, 2, 5], dtype=uint16)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmode2016 = KModes(n_clusters=6, init = \"random\", n_init = 5, verbose=1) \n",
    "kmode2016.fit_predict(df2016subset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2016subset['Cluster'] = kmode2016.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2012 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J, K, M, N, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2012subset = df2012[df2012.columns[pd.Series(df2012.columns).str.startswith('P')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Provide many fewer services', 2, 3, 4, 5, 6, 'Provide many more services', 'NA'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df2012subset['J01'].cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2012subset['J01'].cat.rename_categories(['Provide many fewer services','2','3','4','5','6','Provide many more services','NA'], inplace = True)\n",
    "df2012subset['J02'].cat.rename_categories(['Provide many fewer services','2','3','4','5','6','Provide many more services','NA'], inplace = True)\n",
    "df2012subset['J03'].cat.rename_categories(['Provide many fewer services','2','3','4','5','6','Provide many more services','NA'], inplace = True)\n",
    "df2012subset['J04'].cat.rename_categories(['Government should see to it','2','3','4','5','6','Individuals on own','NA'], inplace = True)\n",
    "df2012subset['J05'].cat.rename_categories(['Government should see to it','2','3','4','5','6','Individuals on own','NA'], inplace = True)\n",
    "df2012subset['J06'].cat.rename_categories(['Government should see to it','2','3','4','5','6','Individuals on own','NA'], inplace = True)\n",
    "df2012subset['J07'].cat.rename_categories(['Government health plan','2','3','4','5','6','Private health plans','NA'], inplace = True)\n",
    "df2012subset['J08'].cat.rename_categories(['Government health plan','2','3','4','5','6','Private health plans','NA'], inplace = True)\n",
    "df2012subset['J09'].cat.rename_categories(['Government health plan','2','3','4','5','6','Private health plans','NA'], inplace = True)\n",
    "df2012subset['J14'].cat.rename_categories(['Regulate business','2','3','4','5','6','Do not regulate', 'NA'], inplace = True)\n",
    "df2012subset['J15'].cat.rename_categories(['Regulate business','2','3','4','5','6','Do not regulate', 'NA'], inplace = True)\n",
    "df2012subset['J16'].cat.rename_categories(['Regulate business','2','3','4','5','6','Do not regulate', 'NA'], inplace = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2012subset['K17'].cat.rename_categories(['High traditionalism','2','3','4','Low traditionalism','NA'], inplace = True)\n",
    "df2012subset['K18'].cat.rename_categories(['Strong commitment','2','3','4','Weak commitment','NA'], inplace = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2012subset['M01'].cat.rename_categories(['Govt should help blacks','2','3','4','5','6','Blacks should help themselves', 'NA'], inplace = True);\n",
    "df2012subset['M02'].cat.rename_categories(['Govt should help blacks','2','3','4','5','6','Blacks should help themselves', 'NA'], inplace = True);\n",
    "df2012subset['M03'].cat.rename_categories(['Govt should help blacks','2','3','4','5','6','Blacks should help themselves', 'NA'], inplace = True);\n",
    "df2012subset['M08'].cat.rename_categories(['High support','2','3','4','Low support','NA'], inplace = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2012subset['N06'].cat.rename_categories(['Greatly decrease','2','3','4','5','6','Greatly increase','NA'], inplace = True);\n",
    "df2012subset['N07'].cat.rename_categories(['Greatly decrease','2','3','4','5','6','Greatly increase','NA'], inplace = True);\n",
    "df2012subset['N08'].cat.rename_categories(['Greatly decrease','2','3','4','5','6','Greatly increase','NA'], inplace = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2012subset['P02'].cat.rename_categories(['High','2','3','4','Low','NA'], inplace = True);\n",
    "df2012subset['P03'].cat.rename_categories(['High','2','3','Low','NA'], inplace = True);\n",
    "df2012subset['P04'].cat.rename_categories(['High','2','3','Low','NA'], inplace = True);\n",
    "df2012subset['P12'].cat.rename_categories(['Little or no difference','2','3','Big difference','NA'], inplace = True);\n",
    "df2012subset['P13'].cat.rename_categories(['Little or no difference','2','3','Big difference','NA'], inplace = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost = [] \n",
    "# K = range(6,7)  \n",
    "# for k in list(K): \n",
    "#     kmode = KModes(n_clusters=k, init = \"random\", n_init = 5, verbose=1) \n",
    "#     kmode.fit_predict(df2012subset) \n",
    "#     cost.append(kmode.cost_) \n",
    "\n",
    "# plt.plot(K, cost, 'x-') \n",
    "# plt.xlabel('No. of clusters') \n",
    "# plt.ylabel('Cost') \n",
    "# plt.title('Elbow Curve') \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 1, iteration: 1/100, moves: 2013, cost: 465598.0\n",
      "Run 1, iteration: 2/100, moves: 669, cost: 464644.0\n",
      "Run 1, iteration: 3/100, moves: 301, cost: 464465.0\n",
      "Run 1, iteration: 4/100, moves: 121, cost: 464410.0\n",
      "Run 1, iteration: 5/100, moves: 73, cost: 464343.0\n",
      "Run 1, iteration: 6/100, moves: 37, cost: 464343.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 2, iteration: 1/100, moves: 2124, cost: 488493.0\n",
      "Run 2, iteration: 2/100, moves: 759, cost: 487003.0\n",
      "Run 2, iteration: 3/100, moves: 604, cost: 486020.0\n",
      "Run 2, iteration: 4/100, moves: 505, cost: 485289.0\n",
      "Run 2, iteration: 5/100, moves: 349, cost: 485086.0\n",
      "Run 2, iteration: 6/100, moves: 209, cost: 484877.0\n",
      "Run 2, iteration: 7/100, moves: 194, cost: 484762.0\n",
      "Run 2, iteration: 8/100, moves: 46, cost: 484762.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 3, iteration: 1/100, moves: 2246, cost: 470880.0\n",
      "Run 3, iteration: 2/100, moves: 929, cost: 468818.0\n",
      "Run 3, iteration: 3/100, moves: 495, cost: 468279.0\n",
      "Run 3, iteration: 4/100, moves: 353, cost: 467987.0\n",
      "Run 3, iteration: 5/100, moves: 78, cost: 467987.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 4, iteration: 1/100, moves: 2202, cost: 490092.0\n",
      "Run 4, iteration: 2/100, moves: 573, cost: 489723.0\n",
      "Run 4, iteration: 3/100, moves: 160, cost: 489639.0\n",
      "Run 4, iteration: 4/100, moves: 118, cost: 489604.0\n",
      "Run 4, iteration: 5/100, moves: 41, cost: 489604.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 5, iteration: 1/100, moves: 2563, cost: 466834.0\n",
      "Run 5, iteration: 2/100, moves: 701, cost: 465931.0\n",
      "Run 5, iteration: 3/100, moves: 289, cost: 465720.0\n",
      "Run 5, iteration: 4/100, moves: 303, cost: 465108.0\n",
      "Run 5, iteration: 5/100, moves: 280, cost: 464720.0\n",
      "Run 5, iteration: 6/100, moves: 206, cost: 464541.0\n",
      "Run 5, iteration: 7/100, moves: 160, cost: 464320.0\n",
      "Run 5, iteration: 8/100, moves: 238, cost: 464065.0\n",
      "Run 5, iteration: 9/100, moves: 148, cost: 463996.0\n",
      "Run 5, iteration: 10/100, moves: 8, cost: 463996.0\n",
      "Best run was number 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 2, 2, ..., 2, 0, 3], dtype=uint16)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmode2012 = KModes(n_clusters=6, init = \"random\", n_init = 5, verbose=1) \n",
    "kmode2012.fit_predict(df2012subset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2012subset['Cluster'] = kmode2012.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2020.to_csv('2020_Cluster_Analysis.csv', index=False)\n",
    "df2016subset.to_csv('2016_Cluster_Analysis.csv', index=False)\n",
    "df2012subset.to_csv('2012_Cluster_Analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(data):\n",
    "    cost = [] \n",
    "    K = range(1,6) \n",
    "    for k in list(K): \n",
    "        kmode = KModes(n_clusters=k, init = \"random\", n_init = 5, verbose=0) \n",
    "        kmode.fit_predict(data) \n",
    "        cost.append(kmode.cost_) \n",
    "      \n",
    "    plt.plot(K, cost, 'x-') \n",
    "    plt.xlabel('No. of clusters') \n",
    "    plt.ylabel('Cost') \n",
    "    plt.title('Elbow Curve') \n",
    "    plt.show()\n",
    "    return kmode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_analysis_2020 = cluster(df2020subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_analysis_2016 = cluster(df2016subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_analysis_2012 = cluster(df2012subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmode = KModes(n_clusters=6, init = \"random\", n_init = 5, verbose=1) \n",
    "kmode.fit_predict(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmode.cluster_centroids_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the uniform strategy for the dumy model should result in a roughly 50/50 split between our two choices which is what we see. This will serve as our baseline to compare the following models against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy model to use as baseline\n",
    "dummy_clf = DummyClassifier(strategy = \"uniform\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have that as a baseline we can begin the modeling process. We will start with a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up one hot encoder to use with our categorical data\n",
    "categorical_processing = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", categorical_processing, categorical_columns),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "# Setting up pipeline steps\n",
    "tree_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocess\", preprocessing),\n",
    "        (\"classifier\", DecisionTreeClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "# Fitting pipeline to the training data\n",
    "tree_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "categorical_processing = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", categorical_processing, categorical_columns),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "# Setting up pipeline steps\n",
    "tree_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocess\", preprocessing),\n",
    "        (\"classifier\", DecisionTreeClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "# Fitting pipeline to the training data\n",
    "tree_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting predictions\n",
    "y_pred = tree_pipe.predict(X_train)\n",
    "# Checking accuracy of predictions\n",
    "print(f\"Training data prediction accuracy: \", accuracy_score(y_train, y_pred))\n",
    "\n",
    "# Getting cross validation score \n",
    "print(f\"CV accuracy: {cross_val_score(tree_pipe, X_train, y_train, cv=5, scoring = 'accuracy').mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a starting point that is a good score. Lets see if we can improve on it by tuning the hyper parameters using GridSearchCV  which tries every combonation of parameters looking for the best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Second Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up parameter grid\n",
    "# param_grid = {'classifier__criterion': ['gini', 'entropy', 'log_loss'],               \n",
    "#               'classifier__max_depth': [2, 4, 6, 8, 10, 12]\n",
    "#              }\n",
    "# # Executing gridsearch\n",
    "# gridsearch = GridSearchCV(estimator=tree_pipe,\n",
    "#                           param_grid=param_grid,\n",
    "#                           scoring='accuracy',\n",
    "#                           cv=5,\n",
    "#                           n_jobs = 3\n",
    "#                          )\n",
    "# # Fit the training data\n",
    "# gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Print accuracy score for the best estimator and the best parameters \n",
    "# print(f'Best estimator score: ' + '{:.4%}'.format(gridsearch.score(X_train, y_train)))\n",
    "# print(f'Gridsearch best params: ')\n",
    "# print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Gridsearch score:  0.979367866549605\n",
    "\n",
    "Gridsearch best params: \n",
    "- 'classifier__criterion': 'gini'\n",
    "- 'classifier__max_depth': 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the parameters in the pipeline\n",
    "tree_pipe.set_params(classifier__criterion = 'gini',\n",
    "                     classifier__max_depth = 8,\n",
    "                    )\n",
    "# Refitting pipeline\n",
    "tree_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Updating the parameters in the pipeline\n",
    "# tree_pipe.set_params(classifier__criterion = gridsearch.best_params_['classifier__criterion'],\n",
    "#                      classifier__max_depth = gridsearch.best_params_['classifier__max_depth'],\n",
    "#                     )\n",
    "# # Refitting pipeline\n",
    "# tree_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting predictions from pipeline\n",
    "y_pred = tree_pipe.predict(X_train)\n",
    "\n",
    "# Checking accuracy of predictions\n",
    "print(f\"Training data prediction accuracy: \", accuracy_score(y_train, y_pred))\n",
    "\n",
    "# Getting cross validation score for training data \n",
    "print(f\"CV accuracy: {cross_val_score(tree_pipe, X_train, y_train, cv=5, scoring = 'accuracy').mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some improvement there but the tree is still somewhat over fit. Let’s take a look at the feature importance to get a better sense of what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets feature importances out of the pipeline. Single features are broken up into multiple columns because of \n",
    "# the encoding. This aggregates the importances by feature so high cardinality features are not discounted. \n",
    "\n",
    "def get_feature_importances(pipe):\n",
    "    # Getting feature names\n",
    "    feature_names = pipe[:-1].get_feature_names_out()\n",
    "    # Creating a series with the feature names and their importances \n",
    "    feature_importances = pd.Series(pipe[-1].feature_importances_, index=feature_names).sort_values(ascending=True)\n",
    "    # Creating a pandas datafram with the feature importances\n",
    "    importances = feature_importances.to_frame(name = 'importance').reset_index().rename(columns={\"index\": \"feature\"})\n",
    "    # Slicing the feature names stored in 'feature' to the first three letter which is the original feature name\n",
    "    importances['feature'] = importances['feature'].str.slice(0, 3)\n",
    "    # Grouping and summing the features\n",
    "    importances = importances.groupby('feature').sum()\n",
    "    # Returning a datafram with the feature importances\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting top 10 feature importances for the Tree\n",
    "tree_importances = get_feature_importances(tree_pipe)\n",
    "tree_importances.nlargest(10, columns= 'importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting 10 smallest feature importances\n",
    "tree_importances.nsmallest(10, columns= 'importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summing 100 smallest feature importances\n",
    "tree_importances.nsmallest(100, columns= 'importance').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of the 100 least important features is zero so the tree is not taking those into account. Next we will try a random forest which creates a number of decision trees each using a different random subset of features. This will allow it to use a broader selection of the data and hopefully get better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up one hot encoder to use with our categorical data\n",
    "categorical_processing = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", categorical_processing, categorical_columns),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "# Setting up pipeline steps\n",
    "forest_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocess\", preprocessing),\n",
    "        (\"classifier\", RandomForestClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fitting pipeline to the training data\n",
    "forest_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Getting predictions from pipeline using training data\n",
    "y_pred = forest_pipe.predict(X_train)\n",
    "\n",
    "# Checking accuracy of predictions\n",
    "print(f\"Training data prediction accuracy: \", accuracy_score(y_train, y_pred))\n",
    "\n",
    "# Getting cross validation score for training data \n",
    "print(f\"CV accuracy: {cross_val_score(forest_pipe, X_train, y_train, cv=5, scoring = 'accuracy').mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s a good score for an untuned model but it is over fit. I will try to address that using RandomizedSearchCV and GridSearchCV to tune the hyper parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up the parameters for RandomizedSearchCV to test\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Creating random grid\n",
    "random_grid = {'classifier__n_estimators': n_estimators,\n",
    "               'classifier__max_depth': max_depth,\n",
    "               'classifier__min_samples_split': min_samples_split,\n",
    "               'classifier__min_samples_leaf': min_samples_leaf,\n",
    "               'classifier__bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setting up RandomizedSearchCV \n",
    "# forest_random = RandomizedSearchCV(estimator = forest_pipe,\n",
    "#                                    param_distributions = random_grid,\n",
    "#                                    n_iter = 100,\n",
    "#                                    cv = 3,\n",
    "#                                    verbose=2,\n",
    "#                                    random_state=42,\n",
    "#                                    n_jobs = -1\n",
    "#                                   )\n",
    "# # Fiting random search model\n",
    "# forest_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #checking best parameters\n",
    "# forest_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "- 'classifier__n_estimators': 400\n",
    "- 'classifier__min_samples_split': 5\n",
    "- 'classifier__min_samples_leaf': 2\n",
    "- 'classifier__max_depth': 90\n",
    "- 'classifier__bootstrap': False\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results from our randomized search I constructed this parameter grid to feed into GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up parameter grid\n",
    "# param_grid = {'classifier__n_estimators': [200, 300, 400],\n",
    "#               'classifier__criterion': ['gini', 'entropy', 'log_loss'],               \n",
    "#               'classifier__max_depth': [70, 80, 90],\n",
    "#               'classifier__min_samples_split': [4, 5, 6],\n",
    "#               'classifier__min_samples_leaf': [2, 3, 4],\n",
    "#               'classifier__bootstrap': [False, True]\n",
    "#              }\n",
    "# # Executing gridsearch\n",
    "# gridsearch = GridSearchCV(estimator=forest_pipe,\n",
    "#                           param_grid=param_grid,\n",
    "#                           scoring='accuracy',\n",
    "#                           cv=5,\n",
    "#                           n_jobs = 3\n",
    "#                          )\n",
    "# # Fit the training data\n",
    "# gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the accuracy on train set\n",
    "# print(f'Best estimator score: ' + '{:.4%}'.format(gridsearch.score(X_train, y_train)))\n",
    "# print(f'Gridsearch best params: ')\n",
    "# print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Best estimator score: 99.2098%\n",
    "\n",
    "Gridsearch best params: \n",
    "- 'classifier__bootstrap': True \n",
    "- 'classifier__criterion': 'gini' \n",
    "- 'classifier__max_depth': 70 \n",
    "- 'classifier__min_samples_leaf': 3 \n",
    "- 'classifier__min_samples_split': 4 \n",
    "- 'classifier__n_estimators': 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the parameters in the pipeline\n",
    "forest_pipe.set_params(classifier__bootstrap = True,\n",
    "                       classifier__criterion = 'gini',\n",
    "                       classifier__max_depth = 70,\n",
    "                       classifier__min_samples_leaf = 3,\n",
    "                       classifier__min_samples_split = 4,\n",
    "                       classifier__n_estimators = 300,\n",
    "                      )\n",
    "# Refitting pipeline\n",
    "forest_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Updating the parameters in the pipeline\n",
    "# forest_pipe.set_params(classifier__n_estimators = gridsearch.best_params_['classifier__n_estimators'],\n",
    "#                        classifier__criterion = gridsearch.best_params_['classifier__criterion'],\n",
    "#                        classifier__max_depth = gridsearch.best_params_['classifier__max_depth'],\n",
    "#                        classifier__min_samples_leaf = gridsearch.best_params_['classifier__min_samples_leaf'],\n",
    "#                        classifier__min_samples_split = gridsearch.best_params_['classifier__min_samples_split'],\n",
    "#                        classifier__bootstrap = gridsearch.best_params_['classifier__bootstrap'],\n",
    "#                       )\n",
    "# # Refitting pipeline\n",
    "# forest_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions from pipeline using training data\n",
    "y_pred = forest_pipe.predict(X_train)\n",
    "\n",
    "#Checking accuracy of predictions\n",
    "print(f\"Training data prediction accuracy: \", accuracy_score(y_train, y_pred))\n",
    "\n",
    "#getting cross validation score for training data \n",
    "print(f\"CV accuracy: {cross_val_score(forest_pipe, X_train, y_train, cv=5, scoring = 'accuracy').mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data prediction accuracy:  0.9920983318700615\n",
    "\n",
    "CV accuracy: 0.966198942746548"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight improvement in both directions but the model is still clearly over fit. The RandomizedSearchCV suggested that the model preformed best with a max depth of 90 which is high. We are worried about overfitting our model so we can try to prune our tree by decreasing the max depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Second Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up parameter grid\n",
    "# param_grid = {'classifier__n_estimators': [50, 100, 150, 200],\n",
    "#               'classifier__criterion': ['gini', 'entropy', 'log_loss'],               \n",
    "#               'classifier__max_depth': [4, 6, 8, 10, 12, 14],\n",
    "#               'classifier__bootstrap': [True, False]\n",
    "#              }\n",
    "# # Executing gridsearch\n",
    "# gridsearch = GridSearchCV(estimator=forest_pipe,\n",
    "#                           param_grid=param_grid,\n",
    "#                           scoring='accuracy',\n",
    "#                           cv=5,\n",
    "#                           n_jobs = 3\n",
    "#                          )\n",
    "# # Fit the training data\n",
    "# gridsearch.fit(X_train, y_train)\n",
    "# # Print the accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the accuracy on train set\n",
    "# print(f'Best estimator score: ' + '{:.4%}'.format(gridsearch.score(X_train, y_train)))\n",
    "# print(f'Gridsearch best params: ')\n",
    "# print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Best estimator score: 99.1659%\n",
    "\n",
    "Gridsearch best params: \n",
    "- 'classifier__bootstrap': True\n",
    "- 'classifier__criterion': gini\n",
    "- 'classifier__max_depth': 14\n",
    "- 'classifier__n_estimators': 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the parameters in the pipeline\n",
    "forest_pipe.set_params(classifier__bootstrap = True,\n",
    "                       classifier__criterion = 'gini',\n",
    "                       classifier__max_depth = 14,\n",
    "                       classifier__n_estimators = 150,\n",
    "                      )\n",
    "# Refitting pipeline\n",
    "forest_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Updating the parameters in the pipeline\n",
    "# forest_pipe.set_params(classifier__bootstrap = gridsearch.best_params_['classifier__bootstrap'],\n",
    "#                        classifier__criterion = gridsearch.best_params_['classifier__criterion'],\n",
    "#                        classifier__max_depth = gridsearch.best_params_['classifier__max_depth'],\n",
    "#                        classifier__n_estimators = gridsearch.best_params_['classifier__n_estimators'],\n",
    "#                       )\n",
    "# # Refitting pipeline\n",
    "# forest_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions from pipeline using training data\n",
    "y_pred = forest_pipe.predict(X_train)\n",
    "\n",
    "#Checking accuracy of predictions\n",
    "print(f\"Training data prediction accuracy: \", accuracy_score(y_train, y_pred))\n",
    "\n",
    "#getting cross validation score for training data \n",
    "print(f\"CV accuracy: {cross_val_score(forest_pipe, X_train, y_train, cv=5, scoring = 'accuracy').mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost no change with those scores. min_samples_split and min_samples_leaf can help prevent overfitting so I try to tune those next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Third Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up parameter grid\n",
    "# param_grid = {'classifier__n_estimators': [125, 150, 175],              \n",
    "#               'classifier__max_depth': [12, 14, 16],\n",
    "#               'classifier__min_samples_split': [4, 6, 8, 10],\n",
    "#               'classifier__min_samples_leaf': [3, 4, 5, 6],\n",
    "#              }\n",
    "# # Executing gridsearch\n",
    "# gridsearch = GridSearchCV(estimator=forest_pipe,\n",
    "#                           param_grid=param_grid,\n",
    "#                           scoring='accuracy',\n",
    "#                           cv=5,\n",
    "#                           n_jobs = 3\n",
    "#                          )\n",
    "# # Fit the training data\n",
    "# gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the accuracy on train set\n",
    "# print(f'Best estimator score: ' + '{:.4%}'.format(gridsearch.score(X_train, y_train)))\n",
    "# print(f'Gridsearch best params: ')\n",
    "# print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Best estimator score: 99.1659%\n",
    "\n",
    "Gridsearch best params:\n",
    "\n",
    "- 'classifier__max_depth': 14,\n",
    "- 'classifier__min_samples_leaf': 3,\n",
    "- 'classifier__min_samples_split': 4,\n",
    "- 'classifier__n_estimators': 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the parameters in the pipeline\n",
    "forest_pipe.set_params(classifier__max_depth = 14,\n",
    "                       classifier__min_samples_leaf = 3,\n",
    "                       classifier__min_samples_split = 4,\n",
    "                       classifier__n_estimators = 150,\n",
    "                      )\n",
    "# Refitting pipeline\n",
    "forest_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Updating the parameters in the pipeline\n",
    "# forest_pipe.set_params(classifier__max_depth = gridsearch.best_params_['classifier__max_depth'],\n",
    "#                        classifier__min_samples_leaf = gridsearch.best_params_['classifier__min_samples_leaf'],\n",
    "#                        classifier__min_samples_split = gridsearch.best_params_['classifier__min_samples_split'],\n",
    "#                        classifier__n_estimators = gridsearch.best_params_['classifier__n_estimators'],\n",
    "#                       )\n",
    "# # Refitting pipeline\n",
    "# forest_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting predictions from pipeline using training data\n",
    "y_pred = forest_pipe.predict(X_train)\n",
    "\n",
    "#Checking accuracy of predictions\n",
    "print(f\"Training data prediction accuracy: \", accuracy_score(y_train, y_pred))\n",
    "\n",
    "#getting cross validation score for training data \n",
    "print(f\"CV accuracy: {cross_val_score(forest_pipe, X_train, y_train, cv=5, scoring = 'accuracy').mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iteration resulted in no change from the previous iteration so parameter tuning has gotten me as far as it can. Next I will try incorporating dimensionality reduction using TruncatedSVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up pipeline steps\n",
    "forest_pipe_SVD = Pipeline(\n",
    "    [\n",
    "        (\"preprocess\", preprocessing),\n",
    "        (\"SVD\", TruncatedSVD(n_components = 200)),\n",
    "        (\"classifier\", RandomForestClassifier(max_depth = 12,\n",
    "                                              n_estimators = 200,\n",
    "                                              min_samples_split = 5,\n",
    "                                              min_samples_leaf = 4,\n",
    "                                              bootstrap = True,\n",
    "                                              \n",
    "                                             )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "# Fitting pipeline to the training data\n",
    "forest_pipe_SVD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up parameter grid\n",
    "# param_grid = {'classifier__n_estimators': [175, 200, 225],\n",
    "#               'classifier__max_depth': [8, 10, 12],\n",
    "#               'SVD__n_components' : [10, 100, 200, 300, 1000]\n",
    "#              }\n",
    "# # Executing gridsearch\n",
    "# gridsearch = GridSearchCV(estimator=forest_pipe_SVD, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs = 3)\n",
    "# # Fit the training data\n",
    "# gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the accuracy on train set\n",
    "# print(f'Best estimator score: ' + '{:.4%}'.format(gridsearch.score(X_train, y_train)))\n",
    "# print(f'Gridsearch best params: ')\n",
    "# print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "\n",
    "- 'SVD__n_components': 10\n",
    "- 'classifier__max_depth': 10\n",
    "- 'classifier__n_estimators': 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the parameters in the pipeline\n",
    "forest_pipe_SVD.set_params(SVD__n_components = 10,\n",
    "                           classifier__max_depth = 10,\n",
    "                           classifier__n_estimators = 200\n",
    "                          )\n",
    "# Refitting pipeline\n",
    "forest_pipe_SVD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Updating the parameters in the pipeline\n",
    "# forest_pipe_SVD.set_params(SVD__n_components = gridsearch.best_params_['SVD__n_components'],\n",
    "#                            classifier__max_depth = gridsearch.best_params_['classifier__max_depth'],\n",
    "#                            classifier__n_estimators = gridsearch.best_params_['classifier__n_estimators']\n",
    "#                           )\n",
    "# # Refitting pipeline\n",
    "# forest_pipe_SVD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions from pipeline using training data\n",
    "y_pred = forest_pipe_SVD.predict(X_train)\n",
    "\n",
    "#Checking accuracy of predictions\n",
    "print(f\"Training data prediction accuracy: \", accuracy_score(y_train, y_pred))\n",
    "\n",
    "#getting cross validation score for training data \n",
    "print(f\"CV accuracy: {cross_val_score(forest_pipe_SVD, X_train, y_train, cv=5, scoring = 'accuracy').mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores actually got slightly worse. XGBoost is another tree based model that often preforms better than random forests so I will try that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up pipeline steps\n",
    "XGBoost_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocess\", preprocessing),\n",
    "        (\"classifier\", XGBClassifier(random_state=42))\n",
    "    ]\n",
    ")\n",
    "# Fitting pipeline to the training data\n",
    "XGBoost_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Getting predictions from pipeline using training data\n",
    "# y_pred = XGBoost_pipe.predict(X_train)\n",
    "# dd\n",
    "# #Checking accuracy of predictions\n",
    "# print(f\"Training data prediction accuracy: \", accuracy_score(y_train, y_pred))\n",
    "\n",
    "# #getting cross validation score for training data \n",
    "# print(f\"CV accuracy: {cross_val_score(XGBoost_pipe, X_train, y_train, cv=5, scoring = 'accuracy').mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'classifier__learning_rate': [0.1, 0.2, 0.3],\n",
    "#               'classifier__max_depth': [2, 6, 8],\n",
    "#               'classifier__min_child_weight': [1, 2],\n",
    "#               'classifier__subsample': [0.5, 0.7],\n",
    "#               'classifier__n_estimators': [50, 100, 150],\n",
    "#              }\n",
    "# # Executing gridsearch\n",
    "# gridsearch = GridSearchCV(estimator=XGBoost_pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs = 3)\n",
    "# # Fit the training data\n",
    "# gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the accuracy on train set\n",
    "# print(f'Best estimator score: ' + '{:.4%}'.format(gridsearch.score(X_train, y_train)))\n",
    "# print(f'Gridsearch best params: ')\n",
    "# print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best estimator score: 98.1782%\n",
    "\n",
    "Gridsearch best params: \n",
    "\n",
    "- 'classifier__learning_rate': 0.1,\n",
    "- 'classifier__max_depth': 2,\n",
    "- 'classifier__min_child_weight': 1,\n",
    "- 'classifier__n_estimators': 150,\n",
    "- 'classifier__subsample': 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_pipe.set_params(classifier__learning_rate = 0.1,\n",
    "                        classifier__max_depth = 2,\n",
    "                        classifier__min_child_weight = 1,\n",
    "                        classifier__subsample = .5,\n",
    "                        classifier__n_estimators = 150)\n",
    "XGBoost_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost_pipe.set_params(classifier__learning_rate = gridsearch.best_params_['classifier__learning_rate'],\n",
    "#                         classifier__max_depth = gridsearch.best_params_['classifier__max_depth'],\n",
    "#                         classifier__min_child_weight = gridsearch.best_params_['classifier__min_child_weight'],\n",
    "#                         classifier__subsample = gridsearch.best_params_['classifier__subsample'],\n",
    "#                         classifier__n_estimators = gridsearch.best_params_['classifier__n_estimators'])\n",
    "# # Refitting pipeline\n",
    "# XGBoost_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions from pipeline using training data\n",
    "y_pred = XGBoost_pipe.predict(X_train)\n",
    "\n",
    "#Checking accuracy of predictions\n",
    "print(f\"Training data prediction accuracy: \", accuracy_score(y_train, y_pred))\n",
    "\n",
    "#getting cross validation score for training data \n",
    "print(f\"CV accuracy: {cross_val_score(XGBoost_pipe, X_train, y_train, cv=5, scoring = 'accuracy').mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvements with the XGBoost model are slight but it is less over fit and has a higher cross validation score so it is the model I will go with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions from pipeline using testing data\n",
    "y_pred = XGBoost_pipe.predict(X_test)\n",
    "\n",
    "#Checking accuracy of predictions\n",
    "print(f\"Testing data accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My final accuracy score on the test data was 96.84% which is quite good and shows how predictable voting behavior can be. The model is still slightly overfit. This is likely due to the high number of feature resulting from One Hot Encoding. I did try to incorporate dimensionality reduction but it was ineffective. Removing features before the modeling process could help solve the overfitting problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating confusion matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(cf, display_labels=['Trump vote', 'Biden vote']).plot(cmap = plt.cm.cividis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shown in the confusion matrix are in line with what I would expect given the accuracy score and it does not appear that the model is struggling to correctly categorize Biden voters more than Trump voters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have a functional model that can predict how an individual will vote based on the whole dataset, I will analyze how well the model preforms when it only has access to a subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting column dictionary for X_train\n",
    "X_train_dict = get_columns(X_train)\n",
    "# Setting up scoring dictionary\n",
    "score_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through the data by category\n",
    "for key in X_train_dict:\n",
    "    # Getting columns in the category\n",
    "    columns = X_train_dict[key]\n",
    "    # Subsetting the data\n",
    "    X_train_subset = X_train[columns]\n",
    "    # Updating the pipeling\n",
    "    XGBoost_pipe.set_params(preprocess__transformers = [(\"cat\", categorical_processing, columns)])\n",
    "    # Refitting the model\n",
    "    XGBoost_pipe.fit(X_train_subset, y_train)\n",
    "    # Getting the cross validation score\n",
    "    score_dict[key] = {'cross validation score' : cross_val_score(XGBoost_pipe, \n",
    "                                                                          X_train_subset, \n",
    "                                                                          y_train, \n",
    "                                                                          cv=5, \n",
    "                                                                          scoring = 'accuracy').mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to update the category labels\n",
    "Catagory_labels = {'B' : 'Political Engagement',\n",
    "                   'C' : 'Media Trust & Consumption',\n",
    "                   'F' : 'Economy',\n",
    "                   'G' : 'Direction of Country',\n",
    "                   'H' : 'Health Care & Policy',\n",
    "                   'J' : 'Federal Spending',\n",
    "                   'K' : 'Abortion, Guns, Imigration',\n",
    "                   'L' : 'Womens and Gender Issues',\n",
    "                   'M' : 'Race, Diversity & Religious Minorities',\n",
    "                   'N' : 'Security & Foreign Policy',\n",
    "                   'P' : 'Trust in Government',\n",
    "                   'Q' : 'LGBTQ Rights',\n",
    "                   'R' : 'Demographics'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a data frame with updated labels \n",
    "score_df = pd.DataFrame.from_dict(score_dict, orient = 'index')\n",
    "score_df.rename(index = Catagory_labels,inplace=True)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the accurate scores for individual categories a few things jump out. Political Engagement has the lowest score which is not particularly surprising because with the country being so narrowly divided, both parties have similar levels of political engagement. The next lowest score is for the Demographics category. This is unfortunate this information is often available at the state or county level so being able to predict how a state or county will vote based on demographics alone would be useful. However the predictions using this data alone are still fairly accurate at 76.4% \n",
    "\n",
    "Looking at the most accurate categories, the Trust in Government and Health Care & Policy categories have the highest score with both being above 94.5%. This tracks with what we know American Politics at the moment. Democrats and Republicans don’t agree on much when it comes to health care policy. For example, a study from 2020 by a team from the Harvard T.H. Chan School of Public Health found that three quarters of democrats would like the federal government to ensure that all citizens have health insurance. In contrast, 79% of Republicans preferred a healthcare system that relies on private insurance. (https://jamanetwork.com/journals/jama/fullarticle/2777394) \n",
    "\n",
    "The reliability of the Trust in Government category when it comes to predicting ones vote is a bit more troubling. It has long been the case that trust in government declines when an individual’s preferred party is out of power in Washington as one can see in this analysis from Pew: https://www.pewresearch.org/politics/2023/09/19/public-trust-in-government-1958-2023/. However, as we saw following the 2020 election a lack of trust in institutions can quickly turn violent and deadly. The fact that this lack of trust is concentrated on one side of the political spectrum makes that an even more dangerous possibility.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up new pipeline \n",
    "# Preprocessing steps\n",
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", categorical_processing, categorical_columns),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "# Setting up pipeline\n",
    "XGBoost_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocess\", preprocessing),\n",
    "        (\"classifier\", XGBClassifier(random_state=42,\n",
    "                                     learning_rate = 0.1,\n",
    "                                     max_depth = 2,\n",
    "                                     min_child_weight = 1,\n",
    "                                     subsample = .5,\n",
    "                                     n_estimators = 150\n",
    "                                    )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fitting pipeline to the training data\n",
    "XGBoost_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting individual feature importances\n",
    "XGBoost_importances = get_feature_importances(XGBoost_pipe)\n",
    "XGBoost_importances.nlargest(10, columns= 'importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the most important features there are a couple that aren’t surprising. Question P28 asks if the respondent favors the House of Representatives decision to impeach Donald Trump in 2019 and P29 asks if the respondent favors the Senates decision not to convict. Similarly H05 asks if the COVID-19 response was adequate. These were obviously major issues in the 2020 election."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling out second most important feature\n",
    "single_feature = list(XGBoost_importances.nlargest(2, columns= 'importance').index)\n",
    "single_feature.remove('P28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling with a single column\n",
    "X_train_subset = X_train[single_feature]\n",
    "# Updating pipeline\n",
    "XGBoost_pipe.set_params(preprocess__transformers = [(\"cat\", categorical_processing, single_feature)])\n",
    "# Refitting pipeline\n",
    "XGBoost_pipe.fit(X_train_subset, y_train)\n",
    "# Getting new predictions\n",
    "y_pred = XGBoost_pipe.predict(X_train_subset)\n",
    "print(f'cross validation score' , cross_val_score(XGBoost_pipe,\n",
    "                                                  X_train_subset, \n",
    "                                                  y_train, \n",
    "                                                  cv=5, \n",
    "                                                  scoring = 'accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something that is interesting is that the model can predict with 87.1% accuracy who the respondent would vote for based on their opinion of the federal government's response to COVID-19. Taking this just a bit further the model loses very little in terms of accuracy as I restrict the features it has access to. Below I've given it features ranked 11 through 20 and 21 through 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting features 11 through 20 ranked by importance\n",
    "next_10_most_important_features = list(XGBoost_importances.nlargest(20, columns= 'importance').index)[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling with a single column\n",
    "X_train_subset = X_train[next_10_most_important_features]\n",
    "# Updating pipeline\n",
    "XGBoost_pipe.set_params(preprocess__transformers = [(\"cat\", categorical_processing, next_10_most_important_features)])\n",
    "# Refitting pipeline\n",
    "XGBoost_pipe.fit(X_train_subset, y_train)\n",
    "# getting new predictions\n",
    "y_pred = XGBoost_pipe.predict(X_train_subset)\n",
    "print(f'cross validation score' , cross_val_score(XGBoost_pipe,\n",
    "                                           X_train_subset, \n",
    "                                           y_train, \n",
    "                                           cv=5, \n",
    "                                           scoring = 'accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting features 21 through 30 ranked by importance\n",
    "next_10_most_important_features = list(XGBoost_importances.nlargest(30, columns= 'importance').index)[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# modeling with a single column\n",
    "X_train_subset = X_train[next_10_most_important_features]\n",
    "# Updating pipeline\n",
    "XGBoost_pipe.set_params(preprocess__transformers = [(\"cat\", categorical_processing, next_10_most_important_features)])\n",
    "# Refitting pipeline\n",
    "XGBoost_pipe.fit(X_train_subset, y_train)\n",
    "# getting new predictions\n",
    "y_pred = XGBoost_pipe.predict(X_train_subset)\n",
    "print(f'cross validation score' , cross_val_score(XGBoost_pipe,\n",
    "                                           X_train_subset, \n",
    "                                           y_train, \n",
    "                                           cv=5, \n",
    "                                           scoring = 'accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with these less politically charged questions the model still has a very good idea how an individual will vote. For example, one of the columns the model had access to in the final run asked \"how important should science be for decisions about COVID-19?\" another asked \"How much is Iran a threat to the United States?\" These are not inherently political questions and in the U.S. they did not used to be politically relevant. However, in the era of hyper polarization, they can accurately predict how an individual will vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project shows that modeling and predicting voting accurately is possible. Aditionally in the context of the 2020 election its shows the impact of the COVID-19 pandemic on the results. Finally with the accuracy of the predictions from teh Trust in Government category, this project is another data point indicating the troubling divisions and mistrust that exist in American society"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most obvious next step would be to look at data from the 2012 and 2016 elections to see how the issues important to voters have changed. As I mentioned previously the healthcare and policy category was a good predictor for vote choice in 2020. This almost certinly impacted by the COVID-19 pandemic. Analyzing previous elections could help quantify that impact.\n",
    "\n",
    "Demographic data is generally available and as this project demonstrated a somewhat accurate predictor of how an individual will vote. Improving that  accuracy would be very useful to political parties and campaigns. \n",
    "\n",
    "Finally, turnout among elligable voters in 2020 was 66% which is a high in recent U.S. history. Still, a third of eligable voters did not turnout. If we could analize those potential voters and understand why they dont vote political parties could boost turnout among their voters or a nonpartisan group could work to boost turnout in general "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
